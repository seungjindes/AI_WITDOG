"""
AIO -- All Model in One
"""
# import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from transformers import  Wav2Vec2Model
from models.ser_spec import SER_AlexNet , SER_ResNet , SER_EfficientNet , SER_VIT , Efficient_coc


class moel_coc(nn.Module):
    def __init__(self):
        super(moel_coc, self).__init__()
        model = 
        
    def forward(self , x):
        x =  F.relu(self.cnn1(x))
        x =  F.relu(self.cnn2(x))
        x =  F.relu(self.cnn3(x))
        x = x.transpose(2,1)
        x , _ = self.lstm(x)
        x =self.flatten(x)
        x = F.relu(self.linear(x))

        return x
    

    

# __all__ = ['Ser_Model']
class Ser_ResNet101(nn.Module):
    def __init__(self):
        super(Ser_ResNet101, self).__init__()
        
        # CNN for Spectrogram
        self.resnet_model = SER_ResNet(num_classes=4, in_ch=3, pretrained=True)
        
        self.post_spec_dropout = nn.Dropout(p=0.1)
        self.post_spec_layer = nn.Linear(2048, 128) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        
        # LSTM for MFCC        
        self.lstm_mfcc = nn.LSTM(input_size=40, hidden_size=256, num_layers=2, batch_first=True, dropout=0.5,bidirectional = True) # bidirectional = True

        self.post_mfcc_dropout = nn.Dropout(p=0.1)
        self.post_mfcc_layer = nn.Linear(153600, 128) # 40 for attention and 8064 for conv, 32768 for cnn-lstm, 38400 for lstm
        
        # LSTM for Cochelagram
        self.coc_model = moel_coc()
        self.post_coc_dropout = nn.Dropout(p=0.1)

        
        # Spectrogram + MFCC  
        self.post_spec_mfcc_att_dropout = nn.Dropout(p=0.1)
        self.post_spec_mfcc_att_layer = nn.Linear(256, 149) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        self.post_multi_att_layer = nn.Linear(384 , 149)
                        
        # WAV2VEC 2.0
        self.wav2vec2_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
        self.post_wav_dropout = nn.Dropout(p=0.1)
        self.post_wav_layer = nn.Linear(768, 128) # 512 for 1 and 768 for 2
        
        # Combination
        self.post_att_dropout = nn.Dropout(p=0.1)
        self.post_att_layer_1 = nn.Linear(512, 128)
        self.post_att_layer_2 = nn.Linear(128, 128)
        self.post_att_layer_3 = nn.Linear(128, 4)
        
                                                                     
    def forward(self, audio_spec, audio_mfcc, audio_wav , audio_coc):      
        
        # audio_spec: [batch, 3, 256, 384]
        # audio_mfcc: [batch, 300, 40]
        # audio_wav: [32, 48000]
        
        audio_mfcc = F.normalize(audio_mfcc, p=2, dim=2)
        
        # spectrogram - SER_CNN
        audio_spec, output_spec_t = self.resnet_model(audio_spec) # [batch, 256, 6, 6], []
        audio_spec = audio_spec.reshape(audio_spec.shape[0], audio_spec.shape[1], -1) # [batch, 256, 36]  
        
        # audio -- MFCC with BiLSTM
        audio_mfcc, _ = self.lstm_mfcc(audio_mfcc) # [batch, 300, 512]  
        
        audio_spec_ = torch.flatten(audio_spec, 1) # [batch, 9216]  
        audio_spec_d = self.post_spec_dropout(audio_spec_) # [batch, 9216]  
        audio_spec_p = F.relu(self.post_spec_layer(audio_spec_d), inplace=False) # [batch, 128]  
        
        #+ audio_mfcc = self.att(audio_mfcc)
        audio_mfcc_ = torch.flatten(audio_mfcc, 1) # [batch, 153600]  
        audio_mfcc_att_d = self.post_mfcc_dropout(audio_mfcc_) # [batch, 153600]  
        audio_mfcc_p = F.relu(self.post_mfcc_layer(audio_mfcc_att_d), inplace=False) # [batch, 128]  
        
        
        # coc_data [batch, 128]
        audio_coc_p = self.coc_model(audio_coc)
    
        
        multi_f = torch.cat([audio_spec_p , audio_mfcc_p , audio_coc_p] , dim=-1)  #[batch, 384]
    
        multi_d = self.post_spec_dropout(multi_f)
        multi_p = F.relu(self.post_multi_att_layer(multi_d) , inplace=False)# [batch, 384]
        multi_p = multi_p.reshape(multi_p.shape[0] , 1, -1) #[batch , 1 , 149]
        

        # wav2vec 2.0 
    
        audio_wav = self.wav2vec2_model(audio_wav).last_hidden_state # [batch, 149, 768] 
        audio_wav = torch.matmul(multi_p, audio_wav) # [batch, 1, 768] 
        audio_wav = audio_wav.reshape(audio_wav.shape[0], -1) # [batch, 768] 
        #audio_wav = torch.mean(audio_wav, dim=1)
        
        audio_wav_d = self.post_wav_dropout(audio_wav) # [batch, 768] 
        audio_wav_p = F.relu(self.post_wav_layer(audio_wav_d), inplace=False) # [batch, 768] 
        
        ## combine()
        #audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p], dim=-1)  # [batch, 384] 
        #정확도 애매하게 내려감 
        audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p , audio_coc_p], dim=-1)  # [batch, 512] 

        audio_att_d_1 = self.post_att_dropout(audio_att) # [batch, 384] 
        audio_att_1 = F.relu(self.post_att_layer_1(audio_att_d_1), inplace=False) # [batch, 128] 
        audio_att_d_2 = self.post_att_dropout(audio_att_1) # [batch, 128] 
        audio_att_2 = F.relu(self.post_att_layer_2(audio_att_d_2), inplace=False)  # [batch, 128] 
        output_att = self.post_att_layer_3(audio_att_2) # [batch, 4] 
        return output_att
    


class Ser_EfficientNet(nn.Module):
    def __init__(self):
        super(Ser_EfficientNet, self).__init__()
        
        # CNN for Spectrogram
        self.effificentnet = SER_EfficientNet(num_classes=4, in_ch=3, pretrained=True)
        
        self.post_spec_dropout = nn.Dropout(p=0.1)
        self.post_spec_layer = nn.Linear(2048, 128) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        
        # LSTM for MFCC        
        self.lstm_mfcc = nn.LSTM(input_size=40, hidden_size=256, num_layers=2, batch_first=True, dropout=0.5,bidirectional = True) # bidirectional = True

        self.post_mfcc_dropout = nn.Dropout(p=0.1)
        self.post_mfcc_layer = nn.Linear(153600, 128) # 40 for attention and 8064 for conv, 32768 for cnn-lstm, 38400 for lstm
        
        # LSTM for Cochelagram
        self.coc_model = moel_coc()
        self.post_coc_dropout = nn.Dropout(p=0.1)

        
        # Spectrogram + MFCC  
        self.post_spec_mfcc_att_dropout = nn.Dropout(p=0.1)
        self.post_spec_mfcc_att_layer = nn.Linear(256, 149) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        self.post_multi_att_layer = nn.Linear(384 , 149)
                        
        # WAV2VEC 2.0
        self.wav2vec2_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
        self.post_wav_dropout = nn.Dropout(p=0.1)
        self.post_wav_layer = nn.Linear(768, 128) # 512 for 1 and 768 for 2
        
        # Combination
        self.post_att_dropout = nn.Dropout(p=0.1)
        self.post_att_layer_1 = nn.Linear(512, 128)
        self.post_att_layer_2 = nn.Linear(128, 128)
        self.post_att_layer_3 = nn.Linear(128, 4)
        
                                                                     
    def forward(self, audio_spec, audio_mfcc, audio_wav , audio_coc):      
        
        # audio_spec: [batch, 3, 256, 384]
        # audio_mfcc: [batch, 300, 40]
        # audio_wav: [32, 48000]
        
        audio_mfcc = F.normalize(audio_mfcc, p=2, dim=2)
        
        # spectrogram - SER_CNN
        audio_spec, output_spec_t = self.effificentnet(audio_spec) # [batch, 256, 6, 6], []
        audio_spec = audio_spec.reshape(audio_spec.shape[0], audio_spec.shape[1], -1) # [batch, 256, 36]  
        
        # audio -- MFCC with BiLSTM
        audio_mfcc, _ = self.lstm_mfcc(audio_mfcc) # [batch, 300, 512]  
        
        audio_spec_ = torch.flatten(audio_spec, 1) # [batch, 9216]  
        audio_spec_d = self.post_spec_dropout(audio_spec_) # [batch, 9216]  
        audio_spec_p = F.relu(self.post_spec_layer(audio_spec_d), inplace=False) # [batch, 128]  
        
        #+ audio_mfcc = self.att(audio_mfcc)
        audio_mfcc_ = torch.flatten(audio_mfcc, 1) # [batch, 153600]  
        audio_mfcc_att_d = self.post_mfcc_dropout(audio_mfcc_) # [batch, 153600]  
        audio_mfcc_p = F.relu(self.post_mfcc_layer(audio_mfcc_att_d), inplace=False) # [batch, 128]  
        
        
        # coc_data [batch, 128]
        audio_coc_p = self.coc_model(audio_coc)
    
        
        multi_f = torch.cat([audio_spec_p , audio_mfcc_p , audio_coc_p] , dim=-1)  #[batch, 384]
    
        multi_d = self.post_spec_dropout(multi_f)
        multi_p = F.relu(self.post_multi_att_layer(multi_d) , inplace=False)# [batch, 384]
        multi_p = multi_p.reshape(multi_p.shape[0] , 1, -1) #[batch , 1 , 149]
        

        # wav2vec 2.0 
    
        audio_wav = self.wav2vec2_model(audio_wav).last_hidden_state # [batch, 149, 768] 
        audio_wav = torch.matmul(multi_p, audio_wav) # [batch, 1, 768] 
        audio_wav = audio_wav.reshape(audio_wav.shape[0], -1) # [batch, 768] 
        #audio_wav = torch.mean(audio_wav, dim=1)
        
        audio_wav_d = self.post_wav_dropout(audio_wav) # [batch, 768] 
        audio_wav_p = F.relu(self.post_wav_layer(audio_wav_d), inplace=False) # [batch, 768] 
        
        ## combine()
        #audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p], dim=-1)  # [batch, 384] 
        #정확도 애매하게 내려감 
        audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p , audio_coc_p], dim=-1)  # [batch, 512] 

        audio_att_d_1 = self.post_att_dropout(audio_att) # [batch, 384] 
        audio_att_1 = F.relu(self.post_att_layer_1(audio_att_d_1), inplace=False) # [batch, 128] 
        audio_att_d_2 = self.post_att_dropout(audio_att_1) # [batch, 128] 
        audio_att_2 = F.relu(self.post_att_layer_2(audio_att_d_2), inplace=False)  # [batch, 128] 
        output_att = self.post_att_layer_3(audio_att_2) # [batch, 4] 
        return output_att



class Ser_VIT(nn.Module):
    def __init__(self):
        super(Ser_VIT, self).__init__()
        
        # CNN for Spectrogram
        self.vit = SER_VIT(num_classes=4, in_ch=3, pretrained=True)
        
        self.post_spec_dropout = nn.Dropout(p=0.1)
        self.post_spec_layer = nn.Linear(2048, 128) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        
        # LSTM for MFCC        
        self.lstm_mfcc = nn.LSTM(input_size=40, hidden_size=256, num_layers=2, batch_first=True, dropout=0.5,bidirectional = True) # bidirectional = True

        self.post_mfcc_dropout = nn.Dropout(p=0.1)
        self.post_mfcc_layer = nn.Linear(153600, 128) # 40 for attention and 8064 for conv, 32768 for cnn-lstm, 38400 for lstm
        
        # LSTM for Cochelagram
        self.coc_model = moel_coc()
        self.post_coc_dropout = nn.Dropout(p=0.1)

        
        # Spectrogram + MFCC  
        self.post_spec_mfcc_att_dropout = nn.Dropout(p=0.1)
        self.post_spec_mfcc_att_layer = nn.Linear(256, 149) # 9216 for cnn, 32768 for ltsm s, 65536 for lstm l
        self.post_multi_att_layer = nn.Linear(384 , 149)
                        
        # WAV2VEC 2.0
        self.wav2vec2_model = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base-960h")
        self.post_wav_dropout = nn.Dropout(p=0.1)
        self.post_wav_layer = nn.Linear(768, 128) # 512 for 1 and 768 for 2
        
        # Combination
        self.post_att_dropout = nn.Dropout(p=0.1)
        self.post_att_layer_1 = nn.Linear(512, 128)
        self.post_att_layer_2 = nn.Linear(128, 128)
        self.post_att_layer_3 = nn.Linear(128, 4)
        
                                                                     
    def forward(self, audio_spec, audio_mfcc, audio_wav , audio_coc):      
        
        # audio_spec: [batch, 3, 256, 384]
        # audio_mfcc: [batch, 300, 40]
        # audio_wav: [32, 48000]
        
        audio_mfcc = F.normalize(audio_mfcc, p=2, dim=2)
        
        # spectrogram - SER_CNN
        audio_spec, output_spec_t = self.vit(audio_spec) # [batch, 256, 6, 6], []
        audio_spec = audio_spec.reshape(audio_spec.shape[0], audio_spec.shape[1], -1) # [batch, 256, 36]  
        
        # audio -- MFCC with BiLSTM
        audio_mfcc, _ = self.lstm_mfcc(audio_mfcc) # [batch, 300, 512]  
        
        audio_spec_ = torch.flatten(audio_spec, 1) # [batch, 9216]  
        audio_spec_d = self.post_spec_dropout(audio_spec_) # [batch, 9216]  
        audio_spec_p = F.relu(self.post_spec_layer(audio_spec_d), inplace=False) # [batch, 128]  
        
        #+ audio_mfcc = self.att(audio_mfcc)
        audio_mfcc_ = torch.flatten(audio_mfcc, 1) # [batch, 153600]  
        audio_mfcc_att_d = self.post_mfcc_dropout(audio_mfcc_) # [batch, 153600]  
        audio_mfcc_p = F.relu(self.post_mfcc_layer(audio_mfcc_att_d), inplace=False) # [batch, 128]  
        
        
        # coc_data [batch, 128]
        audio_coc_p = self.coc_model(audio_coc)
    
        
        multi_f = torch.cat([audio_spec_p , audio_mfcc_p , audio_coc_p] , dim=-1)  #[batch, 384]
    
        multi_d = self.post_spec_dropout(multi_f)
        multi_p = F.relu(self.post_multi_att_layer(multi_d) , inplace=False)# [batch, 384]
        multi_p = multi_p.reshape(multi_p.shape[0] , 1, -1) #[batch , 1 , 149]
        

        # wav2vec 2.0 
    
        audio_wav = self.wav2vec2_model(audio_wav).last_hidden_state # [batch, 149, 768] 
        audio_wav = torch.matmul(multi_p, audio_wav) # [batch, 1, 768] 
        audio_wav = audio_wav.reshape(audio_wav.shape[0], -1) # [batch, 768] 
        #audio_wav = torch.mean(audio_wav, dim=1)
        
        audio_wav_d = self.post_wav_dropout(audio_wav) # [batch, 768] 
        audio_wav_p = F.relu(self.post_wav_layer(audio_wav_d), inplace=False) # [batch, 768] 
        
        ## combine()
        #audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p], dim=-1)  # [batch, 384] 
        #정확도 애매하게 내려감 
        audio_att = torch.cat([audio_spec_p, audio_mfcc_p, audio_wav_p , audio_coc_p], dim=-1)  # [batch, 512] 

        audio_att_d_1 = self.post_att_dropout(audio_att) # [batch, 384] 
        audio_att_1 = F.relu(self.post_att_layer_1(audio_att_d_1), inplace=False) # [batch, 128] 
        audio_att_d_2 = self.post_att_dropout(audio_att_1) # [batch, 128] 
        audio_att_2 = F.relu(self.post_att_layer_2(audio_att_d_2), inplace=False)  # [batch, 128] 
        output_att = self.post_att_layer_3(audio_att_2) # [batch, 4] 
        return output_att