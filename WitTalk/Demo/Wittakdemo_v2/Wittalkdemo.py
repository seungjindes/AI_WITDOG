from openai import OpenAI
import streamlit as st
import time
import json 
from TTS.tts import voice_generator
import librosa

from tempfile import NamedTemporaryFile
from utils import DataPipeline , get_emotion_using_triton

# #ê°œìš”
# WitTalkì€ ë°˜ë ¤ê²¬ê³¼ì˜ ì†Œí†µì„ ìœ„í•œ ì•±ì¸ WitDogì˜ ì¼ë¶€ ì„œë¹„ìŠ¤ ì…ë‹ˆë‹¤. 
# í•´ë‹¹ í˜ì´ì§€ì—ì„œ BarkToText(BTT)ëŠ” ë°˜ë ¤ê²¬ì˜ ìŒì„±ê³¼ ì •ë³´ë¥¼ í†µí•´ ë°˜ë ¤ê²¬ì˜ í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  ê°ì •ì´ ì–´ë–¤ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
# ë°ëª¨ì—ì„œëŠ” BTTì˜ ì‹¤ì œ ì ìš© ì˜ˆì‹œë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì±—ë´‡ ì„œë²„ ì œê³µ ì„œë¹„ìŠ¤ì¸ Straemlt , ChatGPT API ë¥¼ ìœ„í•œ OpenAI ,
# ì‚¬ìš©ìê°€ ë³´ë‚¸ ë¬¸ìë¥¼ ë‹¨ë§ê¸°ì—ì„œ voice ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ TTSë¥¼ ì œê³µí•˜ê¸° ìœ„í•´, OpenVoiceì˜ ì˜¤í”ˆì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.

# ## í”Œë¡œìš° 
# ì§„í–‰ì€ ì•„ë˜ì™€ ê°™ì´ ì´ë£¨ì–´ ì§‘ë‹ˆë‹¤.
# ë‚˜ì¤‘ì— ë‹¤ì‹œ ì“¸ê²Œì—¬ ã…

## ì§„í–‰ìƒí™©
# ëª¨ë¸ì„ conven featureë¥¼ ì‚¬ìš©í•´ì„œ ì ìš©í•˜ë ¤í–ˆìœ¼ë‚˜ extract_featureë¶€í„° data_utilsê¹Œì§€ ë°”ê¿”ì¤˜ì•¼í•´ì„œ ì ì‹œë³´ë¥˜
# í•™ìŠµì„ ì¢€ë” ì‹œí‚¤ê³  ë‚˜ì„œ ë°ëª¨ë¥¼ ì™„ì„±ì‹œí‚¤ëŠ” ê²Œ ì¢‹ì„ ë“¯ ìµœì„ ìœ¼ë¡œ, ë°”ê¿€ê²Œ ë„ˆë¬´ ë§ìŒ

emot_map = {
        'ê´€ì‹¬': 0,
        'í–‰ë³µ': 1,
        'í¥ë¶„(í™”ë‚¨)': 1,
        'ìš”êµ¬': 2,
        'í˜¸ê¸°ì‹¬': 2,
        'ì˜ˆë¯¼': 3,
        'ê²½ê³„': 4,
        'í™”ë‚¨': 5,
        'ìŠ¬í””': 6,
        'ê³µí¬': 7,
        'ë°˜ê°€ì›€': 8,
        'í•˜ìš¸ë§(í–‰ë³µ)': 9,
        'í•˜ìš¸ë§': 10,
        'ì™¸ë¡œì›€': 11,
        'ì‹ ë‚¨': 12,
    }
 

JSON_PATH = "./asset/wittalkdemo.json"
USER = "ë³„ì´"
DOG_EMO = None
ASSISTANT_ID = "asst_4AcrN8f4LuWzP5DyR8cRdaTG"

with open(JSON_PATH , 'r' , encoding='utf-8') as f:
    json_data = json.load(f)

pipleline = DataPipeline()


# ë°˜ë ¤ê²¬ì˜ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , í•´ë‹¹ ìŒì„±ì— ë§ëŠ” ê°ì • ìƒíƒœë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. 
# ê°ì •ì¶”ì¶œì˜ ê²°ê³¼ëŠ” ì¢Œì¸¡ í•˜ë‹¨ì— ì²´í¬ë°•ìŠ¤ì— í‘œê¸°ë©ë‹ˆë‹¤.
# ê°ì •ì¶”ì¶œì€ Bark2text ëª¨ë¸ì´ triton ì„œë²„ì—ì„œ ì¶”ë¡ í•˜ê²Œë©ë‹ˆë‹¤. ë”°ë¼ì„œ triton ì„œë²„ê°€ ë™ì‘í•˜ê³  ìˆì–´ì•¼í•©ë‹ˆë‹¤.
upload_dogsound = st.file_uploader("Upload an dog audio(.wav) file", type=["wav"])

if upload_dogsound is not None:
    with NamedTemporaryFile(suffix=".wav") as temp:
        file_name = upload_dogsound.name
        (species, emotion , sound , target , age , num , index)  =  file_name.split('_')

        temp.write(upload_dogsound.getvalue())
        temp.seek(0)
        temp_dogsound  ,sr = librosa.load(temp  ,sr = 16000)
        #st.audio(temp_dogsound , sample_rate = 16000)
        input_data= pipleline.execute(temp_dogsound , sr ,species ,emotion, sound , target , age , num)
        DOG_EMO = get_emotion_using_triton(input_data , target)



with st.sidebar:

    # OpenAI Key :
    
    openai_api_key = st.text_input("OpenAI API Key", key="chatbot password", type="password")
    client = OpenAI(api_key=openai_api_key)
    thread_id = json_data[USER]['thread_key']


    #action = st.text_input("ë°˜ë ¤ê²¬ì˜ í˜„ì¬ ìƒí™©" , key = "action")

    sample_sound , sr= librosa.load('./TTS/resources/example_reference2.mp3' , sr = 44100)
    st.audio(sample_sound ,format = "audio/mp3" , sample_rate = 44100)

    st.caption("ìœ„ ì˜¤ë””ì˜¤ëŠ” ì‚¬ìš©ì ëª©ì†Œë¦¬ ìƒ˜í”Œì…ë‹ˆë‹¤.")


    # check boxë¥¼ í†µí•´ ë°˜ë ¤ê²¬ì˜ í˜„ì¬ ê°ì •ì„ ë³´ì—¬ì¤Œ 
    st.write('ë°˜ë ¤ê²¬ í˜„ì¬ ê°ì •')

    #def display_emotions(DOG_EMO):

    interest = st.checkbox('ê´€ì‹¬' , value = (DOG_EMO == 'ê´€ì‹¬') , disabled = True)
    happy = st.checkbox('í–‰ë³µí•´', value=(DOG_EMO == 'í–‰ë³µ'), disabled=True)
    very_angry = st.checkbox('ë§¤ìš°í™”ë‚¨', value=(DOG_EMO == 'í¥ë¶„(í™”ë‚¨)'), disabled=True)
    sad = st.checkbox('ìŠ¬í¼', value=(DOG_EMO == 'ìŠ¬í””'), disabled=True)
    angry = st.checkbox('í™”ë‚œë‹¤', value=(DOG_EMO == 'í™”ë‚¨'), disabled=True)
    fear = st.checkbox('ë‘ë µë‹¤', value=(DOG_EMO == 'ê³µí¬'), disabled=True)
    curious = st.checkbox('í˜¸ê¸°ì‹¬', value=(DOG_EMO == 'í˜¸ê¸°ì‹¬'), disabled=True)
    demand = st.checkbox('ìš”êµ¬', value=(DOG_EMO == 'ìš”êµ¬'), disabled=True)
    sensitive = st.checkbox('ì˜ˆë¯¼', value=(DOG_EMO == 'ì˜ˆë¯¼'), disabled=True)
    alert = st.checkbox('ê²½ê³„', value=(DOG_EMO == 'ê²½ê³„'), disabled=True)
    welcoming = st.checkbox('ë°˜ê°€ì›€', value=(DOG_EMO == 'ë°˜ê°€ì›€'), disabled=True)
    howling_happy = st.checkbox('í•˜ìš¸ë§(í–‰ë³µ)', value=(DOG_EMO == 'í•˜ìš¸ë§(í–‰ë³µ)'), disabled=True)
    howling = st.checkbox('í•˜ìš¸ë§', value=(DOG_EMO == 'í•˜ìš¸ë§'), disabled=True)
    lonely = st.checkbox('ì™¸ë¡œì›€', value=(DOG_EMO == 'ì™¸ë¡œì›€'), disabled=True)
    excited = st.checkbox('ì‹ ë‚¨', value=(DOG_EMO == 'ì‹ ë‚¨'), disabled=True)
    unknown = st.checkbox('ë¶ˆëª…í™•' , value = False , disabled = True)


    if interest:
        emotion = "ê´€ì‹¬ë°›ê³ ì‹¶ì–´"
    elif happy:
        emotion = "í–‰ë³µí•¨"
    elif very_angry:
        emotion = "ë§¤ìš°í™”ë‚¨"
    elif sad:
        emotion = "ìŠ¬í””"
    elif angry:
        emotion = "í™”ë‚¨"
    elif fear:
        emotion = "ë‘ë µë‹¤"
    elif curious:
        emotion = "í˜¸ê¸°ì‹¬"
    elif demand:
        emotion = "ìš”êµ¬"
    elif sensitive:
        emotion = "ì˜ˆë¯¼"
    elif alert:
        emotion = "ê²½ê³„"
    elif welcoming:
        emotion = "ë°˜ê°€ì›€"
    elif howling_happy:
        emotion = "í•˜ìš¸ë§(í–‰ë³µ)"
    elif howling:
        emotion = "í•˜ìš¸ë§"
    elif lonely:
        emotion = "ì™¸ë¡œì›€"
    elif excited:
        emotion = "ì‹ ë‚¨"

    #    return emotion

    # Example usag
    #emotion = display_emotions(DOG_EMO)
    #st.write(f"Detected emotion: {emotion}")

        
st.title("ğŸ¶ WitTalk Demo")
st.subheader(f"í˜„ì¬ ìœ—í†¡ì€ {USER}ì™€ ëŒ€í™”ì¤‘ì´ì—ìš”",divider=True)

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": f"ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” {USER}ì—ìš”. ë©!"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    #í…ìŠ¤íŠ¸ì—ì„œ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë¶€ë¶„
    # í•˜ë‚˜ëŠ” Openaiì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ tts , ê·¸ë¦¬ê³  ë‚˜ì˜ ëª©ì†Œë¦¬ë¥¼ ì…íŒ ttsë‘ê°€ì§€ ë²„ì „ì„ ì œê³µí•œë‹¤.
    tts_speaker , my_spearker = voice_generator(prompt)
    #tts_sound  , sr = librosa.load(tts_speaker , sr = 44100)
    my_sound  , sr= librosa.load(my_spearker , sr = 44100)

    st.write("ì‚¬ìš©ìì˜ ì±„íŒ…ì— ëŒ€í•´ ì‚¬ìš©ì ìŒì„±ì…íŒ TTSë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
    #st.audio(tts_sound , format = "audio/wav" , sample_rate= 44100)
    st.audio(my_sound , format = "audio/wav" , sample_rate= 44100)

    # inference server ì—ì„œ ê°ì •ê°’ emotion
    # actionì€ ì¹´ë©”ë¼ì—ì„œ ë°›ì•„ì™€ì•¼í•¨ motion captureí•´ì„œ ê°•ì•„ì§€ê°€ ìˆëŠ”ì§€ ì—†ëŠ”ì§€

    text_form = f"""   
                    ê°ì •ìƒíƒœ : {emotion}
                    ì±„íŒ… : {prompt}
                    
                """
    print(text_form)    
    
    response = client.beta.threads.messages.create(
        thread_id,
        role="user",
        content= text_form
    )

    run = client.beta.threads.runs.create(
    thread_id=thread_id,
    assistant_id=ASSISTANT_ID
    )

    run_id = run.id

    while True:

        run = client.beta.threads.runs.retrieve(
        thread_id = thread_id,
        run_id = run_id
        )

        if run.status =='completed':
            break
        else : time.sleep(2)

    thread_messages = client.beta.threads.messages.list(thread_id)

    msg = thread_messages.data[0].content[0].text.value
    st.session_state.messages.append({"role" : "assistant" , "content" : msg})
    st.chat_message("assistant").write(msg)

